





def filter_products_by_price(products, min_price, max_price):
    """
    Filter products by price range.
    
    Args:
        products: List of dicts with 'name' and 'price' keys
        min_price: Minimum price (inclusive)
        max_price: Maximum price (inclusive)
    
    Returns:
        List of products within price range
    """
    # AI-generated buggy code below - FIX IT
    # Solution: Changed > and < into >= and <= respectively. 
    filtered = []
    for product in products:
        if product['price'] >= min_price and product['price'] <= max_price:
            filtered.append(product)
    return filtered

# Test your solution here
products = [
    {'name': 'Laptop', 'price': 1000},
    {'name': 'Mouse', 'price': 25},
    {'name': 'Keyboard', 'price': 75},
    {'name': 'Monitor', 'price': 300}
]

result = filter_products_by_price(products, 25, 300)
print("Filtered products:", result)


# Test Cell
def test_question_1():
    products = [
        {'name': 'Laptop', 'price': 1000},
        {'name': 'Mouse', 'price': 25},
        {'name': 'Keyboard', 'price': 75},
        {'name': 'Monitor', 'price': 300}
    ]
    
    # Test inclusive bounds
    result = filter_products_by_price(products, 25, 300)
    expected_names = ['Mouse', 'Keyboard', 'Monitor']
    actual_names = [p['name'] for p in result]
    assert set(actual_names) == set(expected_names), f"Expected {expected_names}, got {actual_names}"
    
    # Test edge case - empty list
    assert filter_products_by_price([], 0, 100) == []
    
    # Test no matches
    assert filter_products_by_price(products, 2000, 3000) == []
    
    print("✓ Question 1 tests passed!")

test_question_1()





import requests
import json
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_user_data(user_id):
    """
    Fetch user data from API with proper error handling.
    
    Args:
        user_id: User ID to fetch
        
    Returns:
        dict: User data if successful, None if any error occurs
    """
    # AI-generated code with poor error handling - IMPROVE IT
    url = f"https://jsonplaceholder.typicode.com/users/{user_id}"
       
    # This timeout applies to both the connection and receiving data.
    REQUEST_TIMEOUT = 5
    response = None
    
    try:
        # Pass the timeout value to the requests.get() method
        response = requests.get(url, timeout=REQUEST_TIMEOUT)
        
        # Raise an HTTPError for bad responses (4xx or 5xx status codes)
        response.raise_for_status() 
        
        # If the request was successful (status code 200-299)
        data = response.json()
        logger.info(f"Successfully fetched data for user ID: {user_id}")
        return data

    except requests.exceptions.Timeout:
        # Handle the specific Timeout exception
        logger.error(f"Timeout occurred after {REQUEST_TIMEOUT} seconds for user ID: {user_id}")
        return None
        
    except requests.exceptions.ConnectionError:
        # Handle network-related errors (DNS failure, refused connection, etc.)
        logger.error(f"Connection error occurred for user ID: {user_id}. URL: {url}")
        return None
        
    except requests.exceptions.HTTPError as err:
        # Handle HTTP errors (4xx client error or 5xx server error)
        logger.error(f"HTTP error occurred for user ID: {user_id}. Status code: {response.status_code}. Error: {err}")
        return None
        
    except requests.exceptions.RequestException as e:
        # Handle any other request-related errors (a base class for all exceptions)
        logger.error(f"An unknown request error occurred for user ID: {user_id}. Error: {e}")
        return None
    
    except json.JSONDecodeError:
        # Handle cases where the response is not valid JSON
        logger.error(f"Failed to decode JSON response for user ID: {user_id}. Response text: {response.text[:100]}...")
        return None

    response = requests.get(url)
    data = response.json()
    return data

# Test your solution here
# user_data = get_user_data(1)
# print("User data:", user_data)


# Test Cell
import unittest.mock as mock

def test_question_2():
    # Test successful request
    user_data = get_user_data(1)
    assert user_data is not None
    assert 'name' in user_data
    
    # Test invalid user ID
    user_data = get_user_data(999999)
    assert user_data is None
    
    # Test with mock to simulate network error
    with mock.patch('requests.get') as mock_get:
        mock_get.side_effect = requests.exceptions.RequestException("Network error")
        result = get_user_data(1)
        assert result is None
    
    # Test with mock to simulate timeout
    with mock.patch('requests.get') as mock_get:
        mock_get.side_effect = requests.exceptions.Timeout("Timeout")
        result = get_user_data(1)
        assert result is None
    
    print("✓ Question 2 tests passed!")

test_question_2()


# I added tests for Json Parsing and HTTP Errors and Connection Erros
# assert_called_once() makes sure that our mock function was called. Not absolutely necessary. 
def test_question_2_2():
    mock_response = mock.Mock()
    mock_response.json.side_effect = json.JSONDecodeError("Failed to parse json", doc='{}', pos=0)
    mock_response.text = "Sorry bud, I didn't get a valid JSON to work with :( "
    # JSON Parsing error handling
    with mock.patch('requests.get', return_value = mock_response) as mock_get:
        result = get_user_data(1)
        mock_get.assert_called_once()
        assert result is None
        print("Json Parsing error is being caught properly")

    # HTTP Errors 4xx, 5xx
    mock_response.status_code = '404'
    mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError("404, resource NOT FOUND")
    with mock.patch('requests.get', return_value = mock_response) as mock_get:
        result = get_user_data(1)
        mock_get.assert_called_once()
        assert result is None
        print("HTTP errors are being caught properly")

    # Connection Error . . request.get() crashes
    with mock.patch('requests.get') as mock_get:
        mock_get.side_effect = requests.exceptions.ConnectionError("Couldn't connect. Please Check your Internet connection")
        result = get_user_data(1)
        mock_get.assert_called_once()
        assert result is None
        print("Connection Errors are being caught properly")

test_question_2_2()





class TaskManager:
    """
    A simple task manager for tracking todo items.
    """
    import itertools
    id_counter = itertools.count(1)
    
    def __init__(self):
        """Initialize empty task manager."""
        self.tasks = []
    
    def add_task(self, description, priority=2):
        """
        Add a new task.
        
        Args:
            description (str): Task description
            priority (int): Priority level (1=high, 2=medium, 3=low)
        """
        if not 1<= priority <=3:
            raise ValueError("Priority must be 1, 2 or 3")
        new_task = {
            "id": next(self.id_counter),
            "description": description,
            "priority": priority, 
            "completed": False
        }
        self.tasks.append(new_task)
    
    def complete_task(self, task_id):
        """
        Mark a task as complete.
        
        Args:
            task_id: Unique identifier for the task
            
        Returns:
            bool: True if task was found and completed, False otherwise
        """
        for task in self.tasks:
            if task_id==task['id'] :
                task["completed"]= True
                return True
        return False
            
    
    def get_tasks(self, completed=None, priority=None):
        """
        Get tasks filtered by status and/or priority.
        
        Args:
            completed (bool, optional): Filter by completion status
            priority (int, optional): Filter by priority level
            
        Returns:
            list: List of matching tasks
        """
        tasks_list = self.tasks

        if completed is not None:
            tasks_list = [x for x in tasks_list if x['completed']==completed]

        if priority is not None:
            tasks_list = [x for x in tasks_list if x['priority']==priority]

        return tasks_list
    
    def get_task_count(self, completed=None):
        """
        Get count of tasks by completion status.
        
        Args:
            completed (bool, optional): Count completed (True) or pending (False) tasks
            
        Returns:
            int: Number of matching tasks
        """
        tasks_list = self.tasks
        if completed is not None:
            tasks_list = [x for x in self.tasks if x['completed']==completed]
        return len(tasks_list)

tm = TaskManager()
tm.add_task("Fix bug in login", 1)  # High priority
tm.add_task("Update documentation", 3)  # Low priority
tm.add_task("Code review", 2)  # Medium priority

print("All tasks:", len(tm.get_tasks()))
print("High priority tasks:", len(tm.get_tasks(priority=1)))


# Test Cell
def test_question_3():
    tm = TaskManager()
    
    # Test adding tasks
    tm.add_task("Task 1", 1)
    tm.add_task("Task 2", 2)
    tm.add_task("Task 3", 3)
    
    # Test get all tasks
    all_tasks = tm.get_tasks()
    assert len(all_tasks) == 3
    
    # Test priority filtering
    high_priority = tm.get_tasks(priority=1)
    assert len(high_priority) == 1
    
    # Test task completion
    task_id = all_tasks[0]['id']  # Assuming tasks have 'id' field
    success = tm.complete_task(task_id)
    assert success == True
    
    # Test completion filtering
    completed_tasks = tm.get_tasks(completed=True)
    assert len(completed_tasks) == 1
    
    pending_tasks = tm.get_tasks(completed=False)
    assert len(pending_tasks) == 2
    
    # Test task counts
    assert tm.get_task_count() == 3
    assert tm.get_task_count(completed=True) == 1
    assert tm.get_task_count(completed=False) == 2
    
    print("✓ Question 3 tests passed!")

test_question_3()





def find_common_elements_slow(lists):
    """
    Find elements that appear in ALL provided lists.
    AI-generated inefficient version - OPTIMIZE THIS!
    
    Args:
        lists: List of lists to find common elements in
        
    Returns:
        list: Elements that appear in all lists
    """
    if not lists:
        return []
    
    common = []
    for item in lists[0]:
        is_common = True
        for other_list in lists[1:]:
            found = False
            for other_item in other_list:
                if item == other_item:
                    found = True
                    break
            if not found:
                is_common = False
                break
        if is_common and item not in common:
            common.append(item)
    
    return common

# Optimized version - implement this
def find_common_elements_fast(lists):
    """
    Find elements that appear in ALL provided lists.
    Optimized version with better time complexity.
    
    Args:
        lists: List of lists to find common elements in
        
    Returns:
        list: Elements that appear in all lists
    """
    # Your optimized implementation here
    if not lists:
        return []
        
    # def recursive_set(l:list)->list:
    #     if not l:
    #         return {0,1,2,3,4,5,6,7,8}
    #     return list(set(l[0]) & set(recursive_set(l[1:]))) # Average Time Taken 0.0009360313415527344 seconds.
    
    # return list(recursive_set(lists))
    
    lists = [set(x) for x in lists]
    def recursive_set(l: list):
        if not l:
            return {0,1,2,3,4,5,6,7,8,9}
        return l[0] & recursive_set(l[1:])  # Average Time Taken 0.0007562637329101562 seconds. This approach has less type conversions.

    return list(recursive_set(lists))
 
# Test both versions
test_lists = [
    [1, 2, 3, 4, 5],
    [3, 4, 5, 6, 7],
    [4, 5, 7, 8, 9]
]

print("Slow version:", find_common_elements_slow(test_lists))
print("Fast version:", find_common_elements_fast(test_lists))


# Test Cell

import time

def test_question_4():
    # Basic functionality test
    test_lists = [
        [1, 2, 3, 4, 5],
        [3, 4, 5, 6, 7],
        [4, 5, 7, 8, 9]
    ]
    
    slow_result = find_common_elements_slow(test_lists)
    fast_result = find_common_elements_fast(test_lists)
    
    assert set(slow_result) == set(fast_result), "Results don't match"
    assert set(fast_result) == {4, 5}, f"Expected {{4, 5}}, got {set(fast_result)}"
    
    # Edge cases
    assert find_common_elements_fast([]) == []
    assert find_common_elements_fast([[1, 2], []]) == []
    assert find_common_elements_fast([[1, 2, 3]]) == [1, 2, 3]
    
    # Performance test (rough)
    large_lists = [[i for i in range(1000)] for _ in range(10)]

    # Testing fast time
    start_time = time.time()
    find_common_elements_fast(large_lists)
    fast_time = time.time() - start_time

    # Testing slow time
    start_time = time.time()
    find_common_elements_slow(large_lists)
    slow_time = time.time() - start_time
    
    # Fast version should complete in reasonable time
    assert fast_time < 1.0, "Optimized version is still too slow"

    print(f"Time Taken SLOW : {slow_time}")
    print(f"Time Taken FAST : {fast_time}")
    print("✓ Question 4 tests passed!")

test_question_4()





def calculate_stats(numbers):
    """
    Calculate basic statistics for a list of numbers.
    AI code that works for happy path but fails on edge cases - FIX IT!
    
    Args:
        numbers: List of numbers
        
    Returns:
        dict: Statistics including mean, median, mode, std_dev
    """
    if not numbers:
        return {
            "error":"The numbers list is empty"
        }
    cleaned_numbers = []
    """
    For invalid values:
    1. They are numbers string format, we'll try to convert them to float.
    2. If they are already either int or float, then we'll continue.
    3. Other we'll skip those values and print a message for the same. 
    """
    for n in numbers:
        try:
            if isinstance(n, str):
                cleaned_numbers.append(float(n))
            elif isinstance(n,(int,float)):
                cleaned_numbers.append(float(n))
        except ValueError:
            print(f"{n} is invalid. Skipping it.")


    numbers = cleaned_numbers
    # Sort for median calculation
    sorted_nums = sorted(numbers)
    
    # Mean
    mean = sum(numbers) / len(numbers)
    
    # Median
    n = len(numbers)
    if n % 2 == 0:
        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2
    else:
        median = sorted_nums[n//2]
    
    # Mode (most frequent)
    from collections import Counter
    counts = Counter(numbers)
    mode = counts.most_common(1)[0][0]
    
    # Standard deviation
    variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)
    std_dev = variance ** 0.5
    
    return {
        'mean': mean,
        'median': median,
        'mode': mode,
        'std_dev': std_dev,
        'count': len(numbers)
    }

# Test your solution
test_cases = [
    [1, 2, 3, 4, 5],           # Normal case
    [],                        # Empty list
    [1],                       # Single item
    [1, 1, 1],                # All same
    [1, 'invalid', 3],         # Mixed types
    [1, 2, None, 4]           # None values
]

for i, case in enumerate(test_cases):
    print(f"Test case {i+1}: {case}")
    try:
        result = calculate_stats(case)
        print(f"  Result: {result}")
    except Exception as e:
        print(f"  Error: {e}")
    print()


# Test Cell
def test_question_5():
    # Normal case
    result = calculate_stats([1, 2, 3, 4, 5])
    assert result['mean'] == 3.0
    assert result['median'] == 3.0
    assert result['count'] == 5
    
    # Single item
    result = calculate_stats([42])
    assert result['mean'] == 42
    assert result['median'] == 42
    assert result['mode'] == 42
    assert result['std_dev'] == 0
    
    # Empty list - should handle gracefully
    result = calculate_stats([])
    assert 'error' in result or all(v is None or v == 0 for v in result.values())
    
    # Mixed types - should handle gracefully
    result = calculate_stats([1, 'invalid', 3])
    assert 'error' in result or result['count'] == 2  # Only valid numbers counted
    
    # All same values
    result = calculate_stats([5, 5, 5, 5])
    assert result['mean'] == 5
    assert result['std_dev'] == 0
    
    print("✓ Question 5 tests passed!")

test_question_5()





import pandas as pd
import numpy as np

def analyze_sales_data(df, group_by_column):
    """
    Analyze sales data by grouping and calculating statistics.
    
    Args:
        df: DataFrame with columns ['product', 'category', 'sales', 'profit']
        group_by_column: Column name to group by
        
    Returns:
        DataFrame with aggregated statistics
    """
    # TODO: Implement the following logic:
    # 1. Group by the specified column
    # 2. Calculate sum, mean, and count for 'sales' and 'profit'
    # 3. Calculate profit margin (profit/sales) for each group
    # 4. Sort by total sales (descending)  ## Instructions unclear . . The above task descriptions says "Sroting isn't required"
    # 5. Handle any missing values appropriately

    # Handling invalid group_by_column value
    REQUIRED_COLS = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']
    if group_by_column not in df.columns or df.empty:
        return pd.DataFrame(columns=REQUIRED_COLS)
    
    # Handling the missing values. 
    df['sales'] = df['sales'].fillna(0)
    df['profit'] = df['profit'].fillna(0)

    # Setting aggregate functions to run with groupby
    agg_functions = {
        'sales':['sum','mean'],
        'profit':['sum','mean']
    }

    # Grouping the data
    grouped_df = df.groupby(group_by_column).agg(agg_functions)

    # (Flattening Columns) Setting clear and decoupled column names using List Comprehension
    grouped_df.columns = [f'{x[0]}_{x[1]}' for x in grouped_df.columns]  

    # Calculating Profit Margin ... Divide by Zero is already handled by Pandas to be set to NaN
    grouped_df['profit_margin'] = grouped_df['profit_sum']/grouped_df['sales_sum']
    
    return grouped_df

# Create sample data for testing
sample_data = pd.DataFrame({
    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'category': ['Electronics', 'Electronics', 'Clothing', 'Electronics', 'Electronics', 'Clothing', 'Electronics'],
    'sales': [100, 200, 150, 120, np.nan, 180, 110],
    'profit': [20, 50, 30, 25, 40, 35, 22]
})

print("Sample data:")
print(sample_data)
print("\nAnalysis by product:")
result = analyze_sales_data(sample_data, 'product')
print(result)


# Test Cell
def test_question_6():
    # Create test data
    test_data = pd.DataFrame({
        'product': ['A', 'B', 'A', 'B', 'A'],
        'category': ['Cat1', 'Cat2', 'Cat1', 'Cat2', 'Cat1'],
        'sales': [100, 200, 150, 300, 50],
        'profit': [20, 40, 30, 60, 10]
    })
    
    # Test grouping by product
    result = analyze_sales_data(test_data, 'product')
    
    # Check structure
    assert isinstance(result, pd.DataFrame), "Should return DataFrame"
    assert len(result) == 2, "Should have 2 groups (A and B)"
    
    # Check required columns exist
    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']
    for col in required_cols:
        assert col in result.columns, f"Missing column: {col}"
    
    # Check calculations for product A
    product_a = result.loc['A'] if 'A' in result.index else result[result.index == 'A'].iloc[0]
    assert product_a['sales_sum'] == 300, "Product A sales sum should be 300"
    assert product_a['profit_sum'] == 60, "Product A profit sum should be 60"
    
    print("✓ Question 6 tests passed!")

test_question_6()





def process_data(data):
    """Messy AI-generated code that works but needs refactoring - CLEAN IT UP!"""
    result = {}
    for item in data:
        if 'type' in item and item['type'] == 'user':
            if 'active' in item and item['active']:
                if 'age' in item:
                    if item['age'] >= 18:
                        if 'email' in item and '@' in item['email']:
                            category = 'adult'
                            if item['age'] >= 65:
                                category = 'senior'
                            elif item['age'] >= 25:
                                category = 'adult'
                            else:
                                category = 'young_adult'
                            
                            if category not in result:
                                result[category] = {'count': 0, 'emails': [], 'total_age': 0}
                            
                            result[category]['count'] += 1
                            result[category]['emails'].append(item['email'])
                            result[category]['total_age'] += item['age']
    
    # Calculate averages
    for cat in result:
        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']
        del result[cat]['total_age']
    
    return result

# Test data
test_data = [
    {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},
    {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},
    {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},
    {'type': 'admin', 'active': True, 'age': 35, 'email': 'admin@test.com'},
    {'type': 'user', 'active': True, 'age': 20, 'email': 'invalid-email'},
    {'type': 'user', 'active': True, 'age': 40, 'email': 'user4@test.com'},
]

# Your refactored version should produce the same results
original_result = process_data(test_data)
print("Original result:", original_result)

# TODO: Create your clean, refactored version here
"""
I'm going to write separate functions for validating user, determinig category, aggregating data and calculating averages. This is 
for maintaibility and catching errors efficiently. This is definitely a lot more LOC but it's benefits outweighs initial inconvenince.
I'm defining them ouside the process_user_data_clean() as it will easier for testing them later and also these logic can very well
be used in other parts of application. Also, this will keep process_user_data_clean() concise.
"""
from typing import List, Dict, Any, Optional 

# defining type aliases
UserRecord= Dict[str,Any]
CategorySummary= Dict[str, Dict[str, Any]]

def is_valid_user(item: UserRecord) -> Optional[UserRecord]:
    """
    Validates a dictionary item against core user eligibility rules.

    This handles the initial, nested filtering logic (type, active status, age, email)
    and returns the item only if all checks pass.
    """
    # 1. Check for required keys and core conditions early
    if item.get('type') != 'user' or not item.get('active'):
        return None

    age = item.get('age')
    email = item.get('email')

    # 2. Check age and email validity
    if not isinstance(age, int) or age < 18:
        return None
    if not isinstance(email, str) or '@' not in email:
        return None    
    return item

def determine_category(age: int) -> str:
    """Assigns an age-based category string."""
    if age >= 65:
        return 'senior'
    elif age >= 25:
        # Note: The original logic had 'adult' for 25+ and 'young_adult' for 18-24.
        # Merging the two original 'adult' branches into one logical flow.
        return 'adult'
    else: # age is between 18 and 24
        return 'young_adult'

def aggregate_data(data: List[UserRecord]) -> CategorySummary:
    """
    Filters the data and aggregates it into category summaries (count, total age, emails).
    """
    summary: CategorySummary = {}

    for item in data:
        # Use the separate validation function to clean the loop
        valid_user = is_valid_user(item)
        if valid_user is None:
            continue
            
        age = valid_user['age']
        email = valid_user['email']
        
        category = determine_category(age)

        # Initialize the category entry using .setdefault() for clean initialization
        summary.setdefault(category, {'count': 0, 'emails': [], 'total_age': 0})
        
        # Aggregate statistics
        summary[category]['count'] += 1
        summary[category]['emails'].append(email)
        summary[category]['total_age'] += age
        
    return summary

def calculate_averages(summary: CategorySummary) -> CategorySummary:
    """
    Calculates the average age for each category and removes the intermediate 'total_age'.
    """
    # Create a copy to modify without affecting the input dictionary if it were needed elsewhere
    final_result = summary.copy() 
    
    for category in final_result:
        # Check to prevent division by zero, although 'count' should be > 0 here
        if final_result[category]['count'] > 0:
            final_result[category]['avg_age'] = (
                final_result[category]['total_age'] / final_result[category]['count']
            )
        else:
            final_result[category]['avg_age'] = 0
            
        # Clean up the intermediate total age
        del final_result[category]['total_age']
        
    return final_result

def process_user_data_clean(data: List[UserRecord]) -> CategorySummary:
    """
    Processes raw data to summarize statistics (count, emails, avg_age) 
    for valid, active users grouped by age category.

    Args:
        data: A list of user dictionaries.

    Returns:
        A dictionary where keys are age categories (e.g., 'senior', 'adult') 
        and values are summary statistics.
    """
    # 1. Filter and Aggregate
    aggregated_data = aggregate_data(data)
    
    # 2. Final Calculation and Cleanup
    final_summary = calculate_averages(aggregated_data)
    
    return final_summary

# Test both versions
clean_result = process_user_data_clean(test_data)
print("Clean result:", clean_result)


# Test Cell
def test_question_7():
    test_data = [
        {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},
        {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},
        {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},
        {'type': 'user', 'active': True, 'age': 20, 'email': 'user4@test.com'},
    ]
    
    original_result = process_data(test_data)
    clean_result = process_user_data_clean(test_data)
    
    # Results should be functionally equivalent
    assert set(original_result.keys()) == set(clean_result.keys()), "Categories don't match"
    
    for category in original_result:
        assert original_result[category]['count'] == clean_result[category]['count'], f"Count mismatch for {category}"
        assert abs(original_result[category]['avg_age'] - clean_result[category]['avg_age']) < 0.01, f"Average age mismatch for {category}"
    
    print("✓ Question 7 tests passed!")

test_question_7()





def binary_search_buggy(arr, target):
    """
    Binary search implementation with bugs - FIND AND FIX THEM!
    
    Args:
        arr: Sorted list of integers
        target: Value to search for
        
    Returns:
        int: Index of target if found, -1 otherwise
    """
    left = 0
    right = len(arr) -1 # added -1 because the last index isn't len(arr) but one less than that
    
    while left <= right:  # Replace < with <= to ensure middle value is also checked
        mid = (left + right) // 2
        
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid +1  # Added +1 to discard the mid value 
        else:
            right = mid -1 # Added -1 to discard the mid value
    
    return -1

# Test cases
test_arrays = [
    ([1, 3, 5, 7, 9, 11], 7),    # Should find at index 3
    ([1, 3, 5, 7, 9, 11], 1),    # Should find at index 0
    ([1, 3, 5, 7, 9, 11], 11),   # Should find at index 5
    ([1, 3, 5, 7, 9, 11], 6),    # Should return -1
    ([5], 5),                     # Single element found
    ([5], 3),                     # Single element not found
    ([], 5),                      # Empty array
]

for arr, target in test_arrays:
    result = binary_search_buggy(arr, target)
    print(f"Searching for {target} in {arr}: {result}")


# Test Cell
def test_question_8():
    # Test cases with expected results
    test_cases = [
        ([1, 3, 5, 7, 9, 11], 7, 3),      # Found at index 3
        ([1, 3, 5, 7, 9, 11], 1, 0),      # Found at index 0
        ([1, 3, 5, 7, 9, 11], 11, 5),     # Found at index 5
        ([1, 3, 5, 7, 9, 11], 6, -1),     # Not found
        ([1, 3, 5, 7, 9, 11], 0, -1),     # Less than min
        ([1, 3, 5, 7, 9, 11], 12, -1),    # Greater than max
        ([5], 5, 0),                       # Single element found
        ([5], 3, -1),                      # Single element not found
        ([], 5, -1),                       # Empty array
    ]
    
    for arr, target, expected in test_cases:
        result = binary_search_buggy(arr, target)
        assert result == expected, f"Failed for {target} in {arr}: expected {expected}, got {result}"
    
    # Test that it actually uses binary search (check performance)
    large_array = list(range(0, 10000, 2))  # [0, 2, 4, 6, ..., 9998]
    result = binary_search_buggy(large_array, 5000)
    assert result == 2500, "Should find 5000 at index 2500"
    
    print("✓ Question 8 tests passed!")

test_question_8()





import time
import threading
from typing import Any, Optional, Dict, TypedDict
from collections import OrderedDict

class CacheEntry(TypedDict):
    value: Any
    expires: Optional[float]

class SimpleCache:
    """
    AI-generated basic cache - ADD MISSING FEATURES!
    Current features: basic get/set/delete
    Missing: TTL, size limits, LRU eviction, statistics, management methods
    """
    
    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):
        """
        Initialize cache with size limit and default TTL.
        
        Args:
            max_size: Maximum number of items to store
            default_ttl: Default time-to-live in seconds (None = no expiration)
        """
        self.max_size = max_size
        self.default_ttl = default_ttl
        # Basic storage - you need to enhance this. Yes we'll use OrderedList for LRU tracking.
        # stores {'key':{'value':Any, 'expires':float|None}}
        self._data: OrderedDict[str, CacheEntry] = OrderedDict()

        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'expired_cleanups': 0, # Added for clarity in cleanup method
        }
        
        # Threading for safety 
        self._lock = threading.Lock()

        # TODO: Add data structures for:
        # - TTL tracking (when items expire) # expires key in CacheEntry class
        # - LRU tracking (access order) # OrderedList is used for this very reason
        # - Statistics (hits, misses, evictions) # self._stats is maintained

    # HELPER FUNCTIONS


    
    def get(self, key: str) -> Optional[Any]:
        """
        Get value from cache.
        
        Args:
            key: Cache key
            
        Returns:
            Cached value or None if not found/expired
        """
        with self._lock:
            entry = self._data.get(key)
            # 1. Check if key exists
            if entry is None:
                self._stats['misses']+=1
                return None 
            # 2. Check if item has expired (TTL)
            if self._is_expired(key):
                del self._data[key]
                self._stats['misses']+=1
                return None
            # 3. Update LRU order (move to end)
            entry = self._data.pop(key)
            self._data[key] = entry
            # 4. Update hit/miss statistics
            self._stats['hits'] += 1
            return  entry['value']
            # 5. Clean up expired items # already done in #2 . I don't think cleaning up all the expired items is desired 
            # as that will be quite expensive and should rarely be done. cleanup_expired() is defined later for this
            # purpose.
            
        
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """
        Set value in cache.
        
        Args:
            key: Cache key
            value: Value to cache
            ttl: Time-to-live in seconds (overrides default)
        """
        with self._lock:
            # 1. Calculate expiration time if TTL provided
            ttl = ttl if ttl is not None else self.default_ttl
            expiry =None
            if ttl is not None:
                expiry = ttl + time.time()
            # 2. Check if cache is full and evict LRU items
            if self.size()>=self.max_size:
                self._evict_lru(1)
                self._stats['evictions']+=1
            # 3. Store value with metadata
            value = {"value":value,"expires":expiry}
            # 4. Update LRU order #Automatically updated because we are using OrderedDict
            # 5. Update statistics
            self._stats['hits']+=1
            
            self._data[key] = value
        
    def delete(self, key: str) -> bool:
        """Delete key from cache."""
        with self._lock:
            if key in self._data:
                del self._data[key]
                # TODO: Also remove from TTL and LRU tracking
                return True
            return False
    
    # TODO: Implement these missing methods:
    
    def clear(self) -> None:
        """Clear all items from cache."""
        with self._lock:
            self._data = OrderedDict()
    
    def size(self) -> int:
        """Return current number of items in cache."""
        return len(self._data)
    
    def get_stats(self) -> Dict[str, int]:
        """
        Get cache statistics.
        
        Returns:
            Dict with keys: hits, misses, evictions, current_size
        """
        return {
            **self._stats,
            'current_size': self.size()
        }
    
    def cleanup_expired(self) -> int:
        """
        Remove expired items from cache.
        
        Returns:
            Number of items removed
        """
        with self._lock:
            keys = list(self._data.keys()) # This is important because we cannot modify(here, deleting) self._data while looping over it's keys
            count =0
            for data in keys:
                if self._is_expired(data):
                    del self._data[data]
                    count+=1
            return count 
            
    def _evict_lru(self, count:int)-> int:
        '''
            Cleans the Least Recently Used items. Assumes Lock is held.
            # Evicts 'count' LRU items at max.
            # Returns the number of elements evicted 
        '''
        evicted_count = 0 
        for _ in range(count):
            if self.size()<=0:
                break;

            self._data.popitem(last = False) # The decision to use OrderedDict shines here. Removing the oldest element
            self._stats['evictions']+=1
            evicted_count+=1

        return evicted_count
        
    def _is_expired(self, key: str)-> bool:
        '''
            Checks if a cache entry is expired.
            Assumes that lock is held.
        '''
        entry = self._data.get(key)
        if entry is None:
            return True

        expires_at = entry['expires']
        if expires_at is None:
            return False

        return time.time() >= expires_at  # If current time is ahead of expiry time

        
# Test your enhanced implementation
if __name__ == "__main__":
    # Test TTL functionality
    cache = SimpleCache(max_size=3, default_ttl=1)  # 1 second TTL
    
    print("=== Testing TTL ===")
    cache.set("temp_key", "temp_value")
    print(f"Immediately after set: {cache.get('temp_key')}")
    time.sleep(1.1)
    print(f"After TTL expired: {cache.get('temp_key')}")
    
    print("\n=== Testing Size Limits & LRU ===")
    cache.clear()
    cache.set("a", 1, ttl=None)  # No expiration
    cache.set("b", 2, ttl=None)
    cache.set("c", 3, ttl=None)
    print(f"Cache size after adding 3 items: {cache.size()}")
    
    # Access 'a' to make it recently used
    cache.get("a")
    
    # Add 'd' which should evict 'b' (least recently used)
    cache.set("d", 4, ttl=None)
    print(f"After adding 'd': a={cache.get('a')}, b={cache.get('b')}, c={cache.get('c')}, d={cache.get('d')}")
    
    print("\n=== Testing Statistics ===")
    stats = cache.get_stats()
    print(f"Cache statistics: {stats}")
    
    print("\n=== Testing Cleanup ===")
    cache.set("expire_me", "value", ttl=1)
    time.sleep(1.1)
    removed_count = cache.cleanup_expired()
    print(f"Expired items removed: {removed_count}")


# Test Cell 
import time

def test_question_9():
    print("Testing enhanced cache implementation...")
    
    # Test 1: Basic functionality
    cache = SimpleCache(max_size=3, default_ttl=60)
    
    cache.set("key1", "value1")
    cache.set("key2", "value2")

    assert cache.get("key1") == "value1", "Basic get/set failed"
    assert cache.get("key2") == "value2", "Basic get/set failed"
    assert cache.size() == 2, f"Expected size 2, got {cache.size()}"
    
    # Test 2: TTL expiration
    cache.clear()
    cache.set("ttl_key", "ttl_value", ttl=1)  # 1 second TTL
    assert cache.get("ttl_key") == "ttl_value", "TTL key should be accessible immediately"
    
    time.sleep(1.1)  # Wait for expiration
    assert cache.get("ttl_key") is None, "TTL key should be expired and return None"
    
    # Test 3: Size limits and LRU eviction
    cache.clear()
    cache.set("a", 1)
    cache.set("b", 2) 
    cache.set("c", 3)  # Cache is now full (max_size=3)
    
    # Access 'a' to make it recently used
    cache.get("a")
    
    # Add 'd' which should evict 'b' (least recently used)
    cache.set("d", 4)
    
    assert cache.get("a") == 1, "Recently used 'a' should not be evicted"
    assert cache.get("b") is None, "Least recently used 'b' should be evicted"
    assert cache.get("c") == 3, "'c' should still be in cache"
    assert cache.get("d") == 4, "Newly added 'd' should be in cache"
    assert cache.size() == 3, "Cache size should remain at max_size"
    
    # Test 4: Statistics tracking
    cache.clear()
    cache.set("stat_key", "stat_value")
    cache.get("stat_key")  # Hit
    cache.get("nonexistent")  # Miss
    
    stats = cache.get_stats()
    required_stats = ["hits", "misses", "evictions", "current_size"]
    for stat in required_stats:
        assert stat in stats, f"Missing statistic: {stat}"
    
    assert stats["hits"] > 0, "Should have recorded hits"
    assert stats["misses"] > 0, "Should have recorded misses"
    assert stats["current_size"] == 1, "Should track current size"
    
    # Test 5: Manual cleanup
    cache.clear()
    cache.set("expire1", "value1", ttl=1)
    cache.set("expire2", "value2", ttl=1)
    cache.set("keep", "value3", ttl=None)  # No expiration
    
    time.sleep(1.1)  # Wait for expiration
    removed_count = cache.cleanup_expired()
    
    assert removed_count == 2, f"Should have removed 2 expired items, removed {removed_count}"
    assert cache.get("keep") == "value3", "Non-expiring item should remain"
    assert cache.size() == 1, "Only one item should remain after cleanup"
    
    # Test 6: Edge cases
    cache.clear()
    assert cache.size() == 0, "Cache should be empty after clear"
    assert cache.get("nonexistent") is None, "Getting non-existent key should return None"
    assert cache.delete("nonexistent") == False, "Deleting non-existent key should return False"
    
    # Test delete functionality
    cache.set("delete_me", "value")
    assert cache.delete("delete_me") == True, "Deleting existing key should return True"
    assert cache.get("delete_me") is None, "Deleted key should not be accessible"
    
    print("✓ All Question 9 tests passed!")

test_question_9()






import json
from typing import List, Dict, Any, Tuple, Optional, Union

# Component 1: Data Processor (returns dict with specific structure)
class DataProcessor:
    """AI Component 1 - processes raw data and returns structured dict"""
    
    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Process raw data and return structured dict."""
        if not isinstance(raw_data, list):
            raise ValueError("Expected list input")
        
        result = {
            'total_items': len(raw_data),
            'processed_items': [],
            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}
        }
        
        for item in raw_data:
            if isinstance(item, dict) and 'value' in item:
                result['processed_items'].append({
                    'id': item.get('id', 'unknown'),
                    'processed_value': item['value'] * 2,
                    'original_value': item['value'],
                    'status': 'processed'
                })
            else:
                result['processed_items'].append({
                    'id': 'error',
                    'processed_value': 0,
                    'original_value': None,
                    'status': 'failed'
                })
        
        return result

# Component 2: Analytics Engine (expects JSON string, returns tuple)
class AnalyticsEngine:
    """AI Component 2 - performs analytics on data, expects JSON string input"""
    
    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:
        """Analyze data from JSON string, return (summary, metrics) tuple."""
        try:
            data = json.loads(json_data_string)
        except json.JSONDecodeError:
            return None, "Invalid JSON format"
        
        if not isinstance(data, dict) or 'processed_items' not in data:
            return None, "Missing processed_items in data structure"
        
        items = data['processed_items']
        if not isinstance(items, list):
            return None, "processed_items must be a list"
        
        # Extract numeric values for analysis
        values = []
        failed_count = 0
        
        for item in items:
            if isinstance(item, dict) and item.get('status') == 'processed':
                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):
                    values.append(item['processed_value'])
            else:
                failed_count += 1
        
        if not values:
            return None, "No valid numeric data found for analysis"
        
        summary = f"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)"
        metrics = {
            'avg_value': sum(values) / len(values),
            'max_value': max(values),
            'min_value': min(values),
            'total_value': sum(values),
            'success_rate': len(values) / len(items) if items else 0.0
        }
        
        return summary, metrics

# Component 3: Report Generator (expects list of tuples, returns formatted string)
class ReportGenerator:
    """AI Component 3 - generates reports from analytics results"""
    
    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:
        """Generate report from list of (summary, metrics) tuples."""
        if not isinstance(analytics_results_list, list):
            return "Error: Expected list input for report generation"
        
        if not analytics_results_list:
            return "Error: No data provided for report generation"
        
        report_lines = [
            "=" * 50,
            "           ANALYSIS REPORT",
            "=" * 50
        ]
        
        for i, result in enumerate(analytics_results_list):
            if not isinstance(result, tuple) or len(result) != 2:
                report_lines.append(f"\nSection {i+1}: Invalid data format - expected (summary, metrics) tuple")
                continue
            
            summary, metrics = result
            
            if summary is None:
                report_lines.append(f"\nSection {i+1}: Analysis failed")
                report_lines.append(f"  Error: {metrics}")
                continue
            
            report_lines.append(f"\nSection {i+1}: {summary}")
            
            if isinstance(metrics, dict):
                report_lines.append("  Metrics:")
                for key, value in metrics.items():
                    if isinstance(value, float):
                        report_lines.append(f"    {key}: {value:.2f}")
                    else:
                        report_lines.append(f"    {key}: {value}")
            else:
                report_lines.append(f"  Metrics: {metrics}")
        
        report_lines.append("\n" + "=" * 50)
        return "\n".join(report_lines)

# TODO: Implement the integration functions below

def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:
    """
    Convert dictionary to JSON string for AnalyticsEngine.
    
    Args:
        data_dict: Dictionary from DataProcessor
        
    Returns:
        JSON string suitable for AnalyticsEngine
    """
    # TODO: Implement JSON conversion with error handling
    return json.dumps(data_dict)

def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:
    """
    Validate and clean raw input data.
    
    Args:
        raw_data: Input data of any type
        
    Returns:
        Cleaned list of dictionaries
    """
    # TODO: Implement data validation and cleaning
    clean_output_dict_list: List[Dict[str,Any]] = []
    if isinstance(raw_data, list):
        for li in raw_data:
            if not isinstance(li, dict):
                continue
            if 'id' in li and 'value' in li:
                raw_value = li.get('value')
                
                # CHECK 2: Validate and coerce 'value' to a number
                numeric_value: Optional[Union[int, float]] = None
                
                if isinstance(raw_value, (int, float)):
                    numeric_value = raw_value
                elif isinstance(raw_value, str):
                    try:
                        # Attempt to convert string representations of numbers
                        numeric_value = float(raw_value)
                    except ValueError:
                        # If conversion fails, treat as invalid data
                        pass
                        
                # Only append if we successfully found/coerced a numeric value
                if numeric_value is not None:            
                    clean_output_dict_list.append({
                        'id':li.get('id'),
                        'value':li.get('value')
                    })
        return clean_output_dict_list
    raise ValueError(f"Wrong Format. Expects a list. got {type(raw_data).__name__}") 
    
def integrated_pipeline(raw_data_list: List[Any]) -> str:
    """
    Integrate all three components to process data end-to-end.
    
    This function should:
    1. Validate and clean each raw dataset
    2. Process each dataset through DataProcessor
    3. Convert results to format expected by AnalyticsEngine
    4. Run analytics on each processed dataset
    5. Collect all analytics results
    6. Generate final report using ReportGenerator
    7. Handle all errors gracefully
    
    Args:
        raw_data_list: List of raw data sets to process
        
    Returns:
        str: Final report combining all analyses
    """
    # TODO: Implement the complete integration pipeline
    # Consider these steps:
    
    # Step 1: Initialize components
    processor = DataProcessor()
    analytics = AnalyticsEngine()
    reporter = ReportGenerator()
    
    # Step 2: Process each dataset
    
    analytics_results = []
    
    # Step 3: For each raw_data in raw_data_list:
    #   - Validate and clean the data
    for i, raw_data in enumerate(raw_data_list):
            try:
                # 1. Validate and clean the data for ONE dataset
                cleaned_data = validate_and_clean_raw_data(raw_data)
                
                if not cleaned_data:
                    analytics_results.append((None, f"Dataset {i+1} yielded no valid items after cleaning."))
                    continue
                    
                # 2. Process through DataProcessor
                processed_dict = processor.process_data(cleaned_data)
                
                # 3. Convert to JSON for AnalyticsEngine
                json_string = dict_to_json_adapter(processed_dict)
                
                # 4. Run analytics and 5. Collect results
                summary, metrics = analytics.analyze(json_string)
                analytics_results.append((summary, metrics))
                
            except ValueError as e:
                # Handle validation errors
                analytics_results.append((None, f"Data validation/processing error in Dataset {i+1}: {e}"))
            except Exception as e:
                # Catch all unexpected errors
                analytics_results.append((None, f"An unexpected error occurred in Dataset {i+1}: {e}"))
                
        # 6. Generate final report
    return reporter.generate_report(analytics_results)
    # return reporter.generate_report(analytics_results)
    

def create_sample_data() -> List[List[Dict[str, Any]]]:
    """Create sample test data for the pipeline."""
    return [
        # Dataset 1: Normal data
        [
            {'id': 'A1', 'value': 10},
            {'id': 'A2', 'value': 20},
            {'id': 'A3', 'value': 15}
        ],
        # Dataset 2: Smaller dataset
        [
            {'id': 'B1', 'value': 5},
            {'id': 'B2', 'value': 25}
        ],
        # Dataset 3: Mixed data with issues
        [
            {'id': 'C1', 'value': 30},
            {'id': 'C2'},  # Missing value
            {'value': 40},  # Missing id
            {'id': 'C4', 'value': 'invalid'},  # Invalid value type
        ]
    ]

# Test the integration
if __name__ == "__main__":
    print("Testing component integration...")
    
    # Test individual components first
    print("\n=== Testing Individual Components ===")
    
    processor = DataProcessor()
    analytics = AnalyticsEngine()
    reporter = ReportGenerator()
    
    # Test DataProcessor
    test_data = [{'id': 'test', 'value': 10}]
    processed = processor.process_data(test_data)
    print(f"DataProcessor output: {processed}")
    
    # Test AnalyticsEngine
    json_data = json.dumps(processed)
    analysis_result = analytics.analyze(json_data)
    print(f"AnalyticsEngine output: {analysis_result}")
    
    # Test ReportGenerator
    report = reporter.generate_report([analysis_result])
    print(f"ReportGenerator output:\n{report}")
    
    print("\n=== Testing Integrated Pipeline ===")
    
    # Test full pipeline
    sample_datasets = create_sample_data()
    
    try:
        final_report = integrated_pipeline(sample_datasets)
        print("Integration successful!")
        print(final_report)
    except Exception as e:
        print(f"Integration failed: {e}")
        import traceback
        traceback.print_exc()


# Test Cell
def test_question_10():
    print("Testing integrated pipeline...")
    
    # Test 1: Individual component functionality
    processor = DataProcessor()
    analytics = AnalyticsEngine()
    reporter = ReportGenerator()
    
    # Test DataProcessor
    test_data = [{'id': 'test1', 'value': 10}, {'id': 'test2', 'value': 20}]
    processed = processor.process_data(test_data)
    
    assert isinstance(processed, dict), "DataProcessor should return dict"
    assert 'total_items' in processed, "Missing total_items in processed data"
    assert 'processed_items' in processed, "Missing processed_items in processed data"
    assert processed['total_items'] == 2, "Should count items correctly"
    
    # Test AnalyticsEngine
    json_data = json.dumps(processed)
    summary, metrics = analytics.analyze(json_data)
    
    assert summary is not None, "Analytics should return valid summary"
    assert isinstance(metrics, dict), "Analytics should return metrics dict"
    assert 'avg_value' in metrics, "Missing avg_value in metrics"
    
    # Test ReportGenerator
    report = reporter.generate_report([(summary, metrics)])
    
    assert isinstance(report, str), "Report should be string"
    assert "ANALYSIS REPORT" in report, "Report should contain header"
    assert "Section 1" in report, "Report should contain section"
    
    # Test 2: Data validation and cleaning
    cleaned_data = validate_and_clean_raw_data([
        {'id': 'valid', 'value': 10},
        {'value': 20},  # Missing id
        {'id': 'invalid'},  # Missing value
        'invalid_format'  # Wrong format
    ])
    
    assert isinstance(cleaned_data, list), "Should return list"
    # Should handle invalid data gracefully
    
    # Test 3: Integration adapters
    test_dict = {'processed_items': [{'processed_value': 10}]}
    json_str = dict_to_json_adapter(test_dict)
    
    assert isinstance(json_str, str), "Should return JSON string"
    # Should be valid JSON
    parsed = json.loads(json_str)
    assert parsed == test_dict, "Should preserve data structure"
    
    # Test 4: Full pipeline integration
    sample_datasets = [
        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}],
        [{'id': 'B1', 'value': 5}],
        []  # Empty dataset
    ]
    
    final_report = integrated_pipeline(sample_datasets)
    
    assert isinstance(final_report, str), "Pipeline should return string report"
    assert "ANALYSIS REPORT" in final_report, "Should contain report header"
    
    # Should handle multiple sections
    assert "Section 1" in final_report, "Should have first section"
    assert "Section 2" in final_report, "Should have second section"
    
    # Test 5: Error handling
    # Test with invalid input
    error_report = integrated_pipeline([])
    assert isinstance(error_report, str), "Should handle empty input gracefully"
    
    # Test with malformed data
    malformed_report = integrated_pipeline([["not", "a", "dict", "list"]])
    assert isinstance(malformed_report, str), "Should handle malformed data"
    
    # Test 6: Edge cases
    edge_cases = [
        [{'id': 'only_id'}],  # Missing value
        [{'value': 42}],      # Missing id
        [{}],                 # Empty dict
    ]
    
    edge_report = integrated_pipeline(edge_cases)
    assert isinstance(edge_report, str), "Should handle edge cases"
    assert "ANALYSIS REPORT" in edge_report, "Should still generate report structure"
    
    print("✓ All Question 10 tests passed!")

# Run the test
test_question_10()



