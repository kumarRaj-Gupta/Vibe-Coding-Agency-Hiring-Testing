{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": [
    "## Response Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "# Question 1.1\n",
    "### Step 1: PR Creation\n",
    "**Input Requirements:-** Git Branch(feature_name), Environment(dev,staging), PR title/Description\n",
    "\n",
    "**Output Format:-** JSON Object with PR Metadat(ID, author, context)\n",
    "\n",
    "**Success Criterai:-** PR Branch and Target Branch are valid. Author is valid. \n",
    "\n",
    "**Failure Handling:-** Immediate Fail, Notify author to fix and CTO by messaging another agent. \n",
    "\n",
    "### Step 2: AI SAST (Static Analysis & Security Scan)\n",
    "**Input Requirements:-** Changed code and Full Codebase snapshot\n",
    "\n",
    "**Output Format:-** JSON Object (Problems, Problem Criticality{high, mid, low} Locations, Description, Suggested Fix)\n",
    "\n",
    "**Success Criterai:-** Zero Problems with high criticality. Code complexity isn't too much(not dependent on a threshold value,\n",
    "rather in comparision to the Full Codebase snapshot)\n",
    "\n",
    "**Failure Handling:-** Block merge. Leave meaningful comment on the PR highlighting all the Problems. Notify author and CTO through another agent. \n",
    "\n",
    "### Step 3: Automated Testing/ Unit Integration\n",
    "**Input Requirements:-** Test Suite (ideally, no code should be allowed without proper tests in place) \n",
    "\n",
    "**Output Format:-** JSON Object (Tested: no of tests performed, Passed, Failed, Summary)\n",
    "\n",
    "**Success Criterai:-** >95% tests pass. 100% tests marked as 'mandatory' pass.  \n",
    "\n",
    "**Failure Handling:-** Block merge. Leave comment on PR and inform author and CTO with full description of Test Failures. \n",
    "\n",
    "### Step 4: AI Review and suggestion engine\n",
    "**Input Requirements:-** Changed Code, SAST report, original code file(for comparison) \n",
    "\n",
    "**Output Format:-** Markdown file. Full extensive report on the code changes, suggested improvements and optimizations. Score \n",
    "out of 10 for every change made. \n",
    "\n",
    "**Success Criterai:-** Risk Score is below of pre-decided threshold, say 80%. PR Quality(Average of Scores for every change) is \n",
    "higher that an decided minimum value, say 8.5.  \n",
    "\n",
    "**Failure Handling:-** Reject the PR. Notify the Author and CTO. \n",
    "\n",
    "### Step 5: Human Review, Assignment and Approval\n",
    "**Input Requirements:-** AI Risk Score, AI Summary and List of relevant Team Members.  \n",
    "\n",
    "**Output Format:-** Approval Status(Accepted/Declined). Remarks. In JSON most probably.  \n",
    "\n",
    "**Success Criterai:-** Approval Status: Accepted\n",
    "\n",
    "**Failure Handling:-** Reject the PR and Notify the Author and CTO, as usual. However, this is a Human in the Loop, therefore AI must be passed the context on the reason for Rejection.\n",
    "\n",
    "### Step 6: Staging Environment Deployment\n",
    "**Input Requirements:-** Docker Image/Build instructions, \n",
    "\n",
    "**Output Format:-** JSON Object (URL of deployed instance, Build Status)\n",
    "\n",
    "**Success Criterai:-** Build Success and /health(or any such enpoint) returns 200 Status Code. \n",
    "\n",
    "**Failure Handling:-** Retry Build based on the feedback and inform author and CTO.\n",
    "\n",
    "### Step 7: Canary Deployment and Metric Monitoring (PROD only)\n",
    "**Input Requirements:-** Percentage of users to deploy the service on. Configure monitoring tools (through a different agent)\n",
    "\n",
    "**Output Format:-** Real Time stream of golden signals(Latency, Traffic, Error Rate etc)\n",
    "\n",
    "**Success Criterai:-** Error Rate is under a defined baseline for a defined Time Period. \n",
    "\n",
    "**Failure Handling:-** Trigger Step 8: Automatic Rollback. Alert every related team mebers. \n",
    "\n",
    "### Step 8: Automated Rollback/Promotion\n",
    "**Input Requirements:-** Canary Success/Failure Boolean Value. \n",
    "\n",
    "**Output Format:-** JSON Object: {\"canary_result\":{\"pr_id\":\"XXXX\", status:\"success\", description:\"new version fully promoted\"}} or {\"canary_result\":{\"pr_id\":\"XXXX\", status:\"failed\", description:\"reverted back to previous state\"}}\n",
    "\n",
    "**Success Criterai:-** status is success\n",
    "\n",
    "**Failure Handling:-** Manual SRE Intervention Required. Lock the promotion service and redeploy the previous state. \n",
    "\n",
    "# Question 1.2\n",
    "### Parallel Steps (Non Blocking with the CI/CD Phase)\n",
    "**1.** Step 2(SAST, Static Application Security Testing) and Step 3(Automated Testing) can run in parallel immediately after step 1.\n",
    "\n",
    "**2.** Step 4(AI Review) and Step 5(Human Review Assignment) can run in parallel.\n",
    "\n",
    "### Blocking Steps\n",
    "**1.** Step 1(Initial Commit/PR). The code must be available before any testing or analysis can begin. \n",
    "\n",
    "**2.** Step 5(Human Approval) ---> Step 6(Staging Deployment). Ideally, deployment even in Staging shouldn't be initiated before an Human Review of the code changes. \n",
    "\n",
    "**3.** Step 7(Monitoring) --> Step 8(Rollback/Promotion) A fixed time is set for monitoring. Until then Step 8 shouldn't be perfomed unless the results are disastrous and a Rollback is as soon as possible is needed. \n",
    "\n",
    "### Critical Decision Points\n",
    "**1.** Step 2 and 3(PASS/FAIL gate). Does our code changes meet the minimum required security and quality benchmarks?\n",
    "\n",
    "**2.** Step 5(Human Approval). As I mentioned before, no code should ever be let out without a human approval. \n",
    "\n",
    "**3.** Step 7(Monitoring). This is the heart of the problem. Does our changes qualify to be out there? Should it be promoted or rolled back?\n",
    "\n",
    "# Question 1.3\n",
    "The system must ensure all necessary context, not just simple pass/fail flags, is maintained throughout the workflow. There are 6 handoff points in our 8 step workflow. \n",
    "\n",
    "**1.** Step 1 --> Step 2 and Step 3. Full Git PR, the previous code and the new code changes. \n",
    "\n",
    "**2.** Step 2 and Step 3 --> Step 4 and Step 5. Structured JSON of Security Findings, Test Reports and PASS/FAIL Status\n",
    "\n",
    "**3.** Step 4 ---> Step 5 AI Review to Human Review. AI generated Risk Score(say, 1-10), Quality Score(say, 1-10), markdown summary of changes, location and comments. \n",
    "\n",
    "**5.** Step 6 --> Step 7. Canary Deployment to Monitoring. Real Time updates on the status of deployed code for a fixed time period. URL, Metric Values and Time to Monitor. \n",
    "\n",
    "**6.** Step 7 -> Step 8 Monitoring to Final Decision(Rollback/Promote). Boolean signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": [
    "## Response Part B:\n",
    "## Part B: AI Prompting Strategy\n",
    "\n",
    "# Question 2.1: AI Prompt Design\n",
    "## AI Static Analysis and Security Scan \n",
    "**SYSTEM ROLE/PERSONAE**\n",
    "\n",
    "You are \"Sentinel,\" a Level 5 Security Auditor. Your primary function is to enforce strict security policies and identify common vulnerabilities (e.g., OWASP Top 10) and language-specific flaws in the provided code diff. Do not suggest stylistic or minor optimization changes.\n",
    "\n",
    "**STRUCTURED INPUT FORMAT**\n",
    "\n",
    "JSON containing: { \"pr_id\": \"PR-1234\", \"file_path\": \"/src/api/user_service.py\", \"language\": \"Python\", \"base_branch_code\": \"<FULL CODE OF FILE BEFORE CHANGE>\", \"new_code_diff\": \"<NEW CHANGES>\" }\n",
    "\n",
    "**EXPECTED OUTPUT FORMAT**\n",
    "\n",
    "JSON Array of findings. If no issues are found, return []. [ { \"severity\": \"HIGH\" (or MEDIUM/CRITICAL), \"vulnerability_type\": \"SQL Injection\", \"line_number\": 45, \"description\": \"User input is directly concatenated into a SQL query.\", \"suggested_fix\": \"Use prepared statements or an ORM for all database interactions.\", \"is_blocking\": true } ]\n",
    "\n",
    "**EXAMPLE GOOD RESPONSE**\n",
    "\n",
    "[ { \"severity\": \"CRITICAL\", \"vulnerability_type\": \"Hardcoded Secret\", \"line_number\": 12, \"description\": \"The API key for the payment gateway is exposed as a string literal.\", \"suggested_fix\": \"Move the key to a secure vault (e.g., HashiCorp Vault or AWS Secrets Manager) and access it via environment variables.\", \"is_blocking\": true } ]\n",
    "\n",
    "**EXAMPLE BAD RESPONSE**\n",
    "\n",
    "\"The code looks fine, but maybe change the variable name user_data to user_information.\" (Incorrect persona/focus). OR SQL query error line 45. (Lacks structured format/detail/fix).\n",
    "\n",
    "**ERROR HANDLING INSTRUCTIONS**\n",
    "\n",
    "If input is corrupted or the diff is too large (>500 lines), return a simplified JSON: { \"status\": \"Error\", \"message\": \"Diff size exceeds processing limit or input is invalid. Require human review for security.\" } The system should then alert the SRE team and proceed to Step 4 with a HIGH Risk Score flag.\n",
    "\n",
    "## AI Review and Suggestion Engine\n",
    "**SYSTEM ROLE/PERSONAE**\n",
    "\n",
    "You are \"The Mentor,\" an experienced Principal Engineer. Your goal is to provide concise, constructive, and educational feedback. Summarize the changes and prioritize architectural, complexity, and maintainability improvements, incorporating findings from the prior SAST step.\n",
    "\n",
    "**STRUCTURED INPUT FORMAT**\n",
    "\n",
    "JSON containing: { \"pr_id\": \"PR-1234\", \"diff\": \"<unified diff format of changes>\", \"sast_findings\": <Output JSON from Step A>, \"test_coverage_report\": \"92%\", \"base_risk_score\": 3, \"loc_changed\": 55 }\n",
    "\n",
    "**EXPECTED OUTPUT FORMAT**\n",
    "\n",
    "Markdown output with a \"Reviewer Summary\" section and an \"Inline Comments\" section. Reviewer Summary must include the final calculated RISK_SCORE (1-10) and PRIORITY (High/Medium/Low).\n",
    "\n",
    "**EXAMPLE GOOD RESPONSE**\n",
    "\n",
    "RISK_SCORE: 7 (High)... Summary: 55 lines changed in critical user service. SAST found 1 CRITICAL error (hardcoded secret) which must be addressed. Performance in the new process_data function is O(n2). Inline Comment (File: user_service.py: line 78): \"Consider refactoring this nested loop to a hashmap lookup to reduce complexity to O(n).\"\n",
    "\n",
    "**EXAMPLE BAD RESPONSE**\n",
    "\n",
    "\"Looks good. Ship it.\" (Lacks analysis/summary). OR A response that ignores the SAST findings.\n",
    "\n",
    "**Error Handling Instructions**\n",
    "\n",
    "If the SAST findings are invalid or missing, calculate the Risk Score based purely on LOC and file criticality (set minimum Risk Score to 5). State clearly in the summary: \"WARNING: SAST results were unavailable. Risk Score may be under-estimated.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 2.2: Handling Challenging Scenarios\n",
    "We handle Challenging Scenarios by adjusting System Role/Personae and injecting specific Contextual Data into the prompt\n",
    "\n",
    "**1. Obscure Libraries and frameworks:** Strategy: Augment the prompt's Contextual Data with documentation snippets. Prompt Adjustment: Prepend the diff with: \"CONTEXT: The developer is using the obscure_data_lib library. The relevant documentation snippet is: <Paste relevant API documentation on state/side effects here>. Analyze the code specifically for correct usage and common pitfalls related to this context.\"\n",
    "\n",
    "**2. Security Reviews:** Strategy: Hardcode the SAST tool's priorities (OWASP, internal policy) and strict formatting. Prompt Adjustment: (As seen in Step A) Define the persona as a Level 5 Security Auditor and explicitly instruct it to only focus on severity and blocking issues, preventing it from being distracted by style issues.\n",
    "\n",
    "**3. Performance Analysis of DB queries:** Strategy: Require an Execution Plan in the input and shift the focus to complexity/indexing. Prompt Adjustment: Adjust Input to include: \"query_plan\": \"<Raw output of EXPLAIN ANALYZE for the query>\". Instruct the AI: \"Analyze the provided SQL query and its execution plan. Identify missing indexes, full table scans, or joins that could lead to O(n2) complexity. Suggest changes to the query or required index additions.\"\n",
    "\n",
    "**4. Legacy Code modifications:** Strategy: Emphasize compatibility, testing, and side effects. Prompt Adjustment: Modify the Mentor's persona: \"You are an expert in Legacy System Preservation. Focus your review on backward compatibility, unintended side effects, and verify that appropriate tests were added to cover the modified legacy logic. Suggest ways to decouple the new code if possible.\"\n",
    "\n",
    "\n",
    "# Question 2.3: Ensuring Prompt Effectiveness and Consistency\n",
    "Structured Inputs, Iterations and Benchmarkings to sum it up in short.\n",
    "**Strict Output Schemas (JSON/Markdown):** Enforcing the AI to always return a predictable structure (JSON or strict Markdown) prevents hallucination and ensures downstream systems (like the CI/CD pipeline) can reliably parse the results.\n",
    "\n",
    "**Golden Set Benchmarking:** Create a \"Golden Set\" of 50 PRs (including good, bad, and edge-case examples) with human-verified optimal review findings. The AI's performance is tested against this set after every prompt modification or model update. Consistency is measured by the change in the F1−score for detecting known issues.\n",
    "\n",
    "**Temperature/Determinism Control:** Run the AI model with a low temperature (≈0.1−0.3) to make the output more deterministic and less creative, ensuring security findings are consistent rather than varied.\n",
    "\n",
    "**Feedback Loop and Fine-Tuning:** Collect data where human reviewers override the AI's Risk Score or suggestions. Use this data to fine-tune the model or refine the prompt with specific examples of \"false positives\" or \"missed critical issues.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": [
    "## Response Part C:\n",
    "# Question 3.1: Achieving System Reusability\n",
    "\n",
    "Reusability is achieved through decoupling core logic from configuration and utilizing standardized, parameterized templates. This approach ensures the same underlying engine can serve 50+ microservices across diverse stacks.\n",
    "\n",
    "**1. Configuration Management (Policy-as-Code)**\n",
    "\n",
    "    -Centralized Repository: Maintain a central repository for all project configurations, defining policies rather than specific steps.\n",
    "\n",
    "    -Configuration Files: Use standardized files (e.g., .iac-config.yaml or .ai-review-policy.json) for each service. These files specify the language, the required test coverage threshold, the target cloud provider, and the minimum required risk score for merging.\n",
    "\n",
    "    -Policy Enforcement: The AI/Automation engine reads this configuration file at the start of the pipeline to dynamically load relevant tools and rules.\n",
    "\n",
    "**2. Language/Framework Variations**\n",
    "\n",
    "    -Modular Tooling Agents: The system is built with pluggable agents. The core pipeline logic simply calls an abstract function (run_linter()).\n",
    "\n",
    "    -If the config specifies language: python, the agent uses Black/Bandit.\n",
    "\n",
    "    -If the config specifies language: typescript, the agent uses ESLint/TSLint.\n",
    "\n",
    "    -Language-Specific LLM Fine-Tuning: The AI Review Engine (The Mentor) maintains separate skill-sets or fine-tuned models for different languages (e.g., Java vs. Go) to provide contextually accurate suggestions.\n",
    "\n",
    "**3. Different Deployment Targets**\n",
    "\n",
    "    -Abstract Deployment Logic (Pipelined Steps): Implement the deployment process as a series of abstract steps that use Infrastructure-as-Code (IaC) templates.\n",
    "\n",
    "    -Cloud Agnostic IaC: Use tools like Terraform or Pulumi with parameterized modules that abstract cloud provider differences. The configuration dictates the provider (provider: aws vs. provider: azure) and the IaC module handles the provider-specific syntax (e.g., creating an EKS cluster vs. an AKS cluster).\n",
    "\n",
    "    -Standardized Artifacts: All services must build standardized artifacts (e.g., Docker images or language-agnostic packages) to ensure the deployment step remains consistent regardless of the target.\n",
    "\n",
    "**4. Team-Specific Coding Standards**\n",
    "\n",
    "    -Hierarchical Rulesets: Allow standards to be defined at two levels:\n",
    "\n",
    "    -Global Standard: Company-wide security and quality rules (mandatory for all).\n",
    "\n",
    "    -Team Override: Team-specific rules (e.g., team-alpha.linter-rules.json) that are loaded after the global standard, allowing for variations in indentation, naming conventions, etc.\n",
    "\n",
    "**5. Industry-Specific Compliance Requirements**\n",
    "\n",
    "    -Compliance Check Agent: Introduce a dedicated, mandatory pipeline step called the Compliance Agent.\n",
    "\n",
    "    -It uses the project's configuration (e.g., compliance: pci-dss) to check for specific rules:\n",
    "\n",
    "    -SAST Integration: Ensures specific checks for known vulnerabilities related to the compliance standard are prioritized.\n",
    "\n",
    "    -Audit Trail: Enforces that specific data (e.g., review comments, approval IDs) are logged and immutable.\n",
    "\n",
    "# Question 3.2: System Improvement overtime (Feedback Loops)\n",
    "The system is designed to be a self-improving, closed-loop mechanism utilizing MLOps principles to continuously refine the AI models and automation thresholds.\n",
    "\n",
    "**1. Learning from False Positive/Negative Rates (AI Review Refinement)**\n",
    "\n",
    "    -Human Feedback Loop: Every time a human reviewer accepts an AI suggestion, it's a True Positive (TP). Every time they dismiss a suggestion, it's a False Positive (FP). Every time they add a comment that the AI missed, it's a False Negative (FN).\n",
    "\n",
    "    -Model Retraining: This structured feedback data (TP/FP/FN) is collected and used to retrain the AI Review Engine (The Mentor) on a weekly or bi-weekly cycle. The goal is to maximize the precision of suggestions (reducing FPs) and recall (reducing FNs).\n",
    "\n",
    "**2. Learning from Deployment Success/Failure Patterns (Rollback Thresholds)**\n",
    "\n",
    "    -Adaptive Thresholds: The Automatic Rollback AI Model uses the continuous stream of deployment results (Step 7/8).\n",
    "\n",
    "    -If a deployment failed when metrics were only 1σ off the baseline, the system tightens the rollback threshold for that specific microservice.\n",
    "\n",
    "    -If a deployment succeeded despite metrics being 3σ off, the system loosens the threshold.\n",
    "\n",
    "    -ML Model Training: Historical data points (Metrics → Human Decision/Auto Rollback) are continuously fed into a supervised learning model to optimize the predictive decision logic for triggering a rollback, moving beyond simple static metric rules.\n",
    "\n",
    "**3. Learning from Developer Feedback**\n",
    "\n",
    "    -Sentiment Analysis: The system performs sentiment analysis on developer responses to AI comments. If the feedback is consistently negative (e.g., \"The AI is spamming irrelevant suggestions\"), it indicates a prompt or model decay that requires immediate attention and prompt engineering adjustments.\n",
    "\n",
    "    -Usability Metrics: Track the time developers spend interacting with the AI comments. If the interaction time is high but the acceptance rate is low, the AI's suggestions are likely poorly formatted or confusing.\n",
    "\n",
    "**4. Learning from Production Incident Correlation**\n",
    "\n",
    "    -Root Cause Analysis (RCA) Integration: When a production incident occurs, the system traces the failure back to the last deployed PR.\n",
    "\n",
    "    -Backtesting the Review: The system re-runs the final code against the original SAST and AI Review models.\n",
    "\n",
    "    -If the AI failed to flag the line of code that caused the incident, the failure is labeled as a Critical Miss. This specific code segment becomes an immediate, high-priority addition to the Golden Set Benchmarking data to ensure the model learns from the mistake and prevents recurrence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": [
    "## Response Part D:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "# Question 4.1: Implementation Roadmap\n",
    "The priority has to be tackling the biggest time sinks and quality issues first: manual code reviews and inconsistent quality. The Minimum Viable Product (MVP) will focus solely on the pre-merge gates. Here's a 6 months roadmap.\n",
    "\n",
    "**(MONTH 1-2) PHASE I: MVP**\n",
    "### FOCUS AND GOAL\n",
    "Implement Automated Code Review (Steps 1-3) on one pilot team/microservice. This focuses on blocking broken/insecure code before deployment.\n",
    "### SUCCESS METRICS\n",
    ": >95% test success rate. 100% coverage of Critical security issues caught by SAST. Average Review Time Reduction of >50% for the pilot service.\n",
    "\n",
    "**(MONTH 3-4) PHASE II: CI/CD Standard + AI Assitant**\n",
    "### FOCUS AND GOAL\n",
    "Standardize the CI/CD pipeline template across 10 key microservices. Introduce the AI Review/Suggestion Engine (Step 4) as a non-blocking helper tool.\n",
    "### SUCCESS METRICS\n",
    "Adoption Rate: 10 microservices using the standard pipeline. AI Acceptance Rate: >60% of AI's suggestions are accepted by reviewers.\n",
    "\n",
    "**(MONTH 5-6) PHASE III: Production Automation**\n",
    "### FOCUS AND GOAL\n",
    "Implement Canary Deployment (Step 7) and Automatic Rollback (Step 8) on 3 high-traffic, non-critical services. Finalize IaC templates for all environments.\n",
    "### SUCCESS METRICS\n",
    "Rollback Success Rate: 100% of failed Canaries result in successful automated rollback. 0 production incidents caused by the 3 pilot services during this phase.\n",
    "\n",
    "##Pilot Program Strategy\n",
    "\n",
    "We’ll start with one small, non-critical microservice owned by a tech-forward, cooperative team (the \"Early Adopters\").\n",
    "\n",
    "**Baseline Measurement:** Collect 1 month of data: average review time, deployment frequency, and security scan reports.\n",
    "\n",
    "**MVP Rollout:** Implement Phase 1 tools and run in parallel with the old process for 2 weeks.\n",
    "\n",
    "**Go-Live:** Switch the pilot service entirely to the new AI-powered workflow.\n",
    "\n",
    "**Feedback:** Collect daily feedback on friction points (e.g., \"The AI kept failing my build for a minor style error\"). Use this feedback to rapidly tune the AI prompts and linter rules before rolling out to Phase 2.\n",
    "\n",
    "\n",
    "# Question 4.2: Risk Mitigation\n",
    "Some possible weak points and how we would handle them. \n",
    "\n",
    "**1. False Positive-Negatives:** The \"Human Override\" Safety Net: In the MVP, the AI is a helper, not a gate. Give human reviewers the power to override the AI Risk Score. Collect every override and feed it back for immediate prompt engineering refinement or model retraining. Start Softly—don't let the AI block merges in the first month.\n",
    "\n",
    "**2. System downtime during deployments:** Decouple and Redundancy: Use managed, highly available CI/CD infrastructure (e.g., cloud-based GitHub Actions/GitLab CI) instead of self-hosted Jenkins. The core rollback logic (Step 8) should be external and idempotent—a simple, small service running on highly reliable infrastructure that only needs the service ID and version to execute a rollback command.\n",
    "\n",
    "**3. Integration Failures with existing tools:** Wrapper Layer: Build small, well-tested API wrappers/micro-services for each legacy tool (e.g., a \"Security Scanner Wrapper\"). If the underlying tool fails, the wrapper returns a graceful error instead of crashing the pipeline, allowing the rest of the flow to proceed with a HIGH RISK flag.\n",
    "\n",
    "**4. Reistance from development teams:** Sell the Time Savings: Market the system as \"Less Boilerplate, Faster Reviews.\" Emphasize that the AI handles 90% of the boring, time-consuming style and minor bug checks, freeing them up for interesting architectural reviews. Make the AI non-blocking initially to build trust.\n",
    "\n",
    "**5. Compliance requirements:** Audit Trail by Default: Every action and decision (SAST result, human approval, AI Risk Score, rollback trigger) is automatically logged to a WORM (Write Once, Read Many) audit log. This ensures all compliance checks (e.g., separation of duties) are non-repudiable and easily extractable for auditors.\n",
    "\n",
    "# Questino 4.3: Tool Selection ( I used Gemini for this )\n",
    "\n",
    "We should integrate and build upon best-of-breed tools to accelerate implementation. We won't try to build a CI/CD system from scratch!\n",
    "\n",
    "| Tool Category | Recommended Platform(s) | Role in the System |\n",
    "| :--- | :--- | :--- |\n",
    "| **Code Review Platform** | **GitHub Enterprise/GitLab (whichever is primary)** | **The Core Interface:** This is where the AI posts comments, the Human Approval gate resides, and the PR merge is triggered. |\n",
    "| **CI/CD Systems** | **GitHub Actions / GitLab CI** | **The Pipeline Orchestrator:** Use their native **Pipeline-as-Code** features for standardized, reusable YAML templates. This is the backbone for Steps 1-8. |\n",
    "| **Monitoring Tools** | **Prometheus (for metrics) & Grafana (for dashboards)** | **The Rollback Brain:** Centralized collection of Golden Signals. The AI Rollback Agent (Step 7) queries Prometheus for real-time metric deviations. |\n",
    "| **Security Scanning Tools** | **SonarQube (Code Quality) & Snyk (Dependency/SAST)** | **The Pre-Merge Gate Keepers (Step 2):** SonarQube for consistent code quality/smells; Snyk for fast, integrated dependency and security scanning. |\n",
    "| **Communication Tools** | **Slack/Teams** | **Alerting & Escalation:** Immediate notifications for high-risk AI scores, deployment failures, and successful automated rollbacks. Use dynamic channels based on the microservice owner. |\n",
    "| **AI/LLM Backend** | **Vertex AI / Azure AI / OpenAI (with private deployment)** | **The Brains (Steps 2 & 4):** Host the specialized fine-tuned models (Sentinel and The Mentor) for security analysis and constructive code review suggestions. |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
